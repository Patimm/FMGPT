# ğŸ§  FMGPT â€“ RAG-gestÃ¼tzter Football Manager Chatbot

FMGPT ist eine moderne Streamlit-Anwendung, die Retrieval-Augmented Generation (RAG) nutzt, um Football-Manager-Fragen mit eigenen Dokumenten und einer lokalen LLM (Ollama) zu beantworten. Die Architektur ist modular, testbar und auf Automatisierung ausgelegt.

## ğŸš€ Features

- Upload und automatisches Chunking von Textquellen (PDF, TXT, etc.)
- Erstellung und Pflege einer ChromaDB-Vektordatenbank
- Semantische Suche Ã¼ber eingebettete Chunks (Sentence Transformers)
- Kontextuelle RAG-Abfrage mit lokalem LLM (Ollama, z.B. Mistral)
- Interaktives Chat-Interface Ã¼ber Streamlit
- Schutzmechanismen gegen Prompt-Injection (regel- und LLM-basiert)
- Logging und automatisches Speichern des Chatverlaufs
- Modularer, getesteter Python-Code (pytest)

## ğŸ—ƒï¸ Projektstruktur

```text
scripts/
â”œâ”€â”€ app.py                  # Hauptanwendung (Streamlit-UI, RAG, LLM)
â”œâ”€â”€ build_chroma_db.py      # Erstellt Vektordatenbank aus Textquellen
â”œâ”€â”€ chunk_texts.py          # Zerlegt Texte in semantische Chunks
â”œâ”€â”€ embed_chunks.py         # Erstellt Embeddings fÃ¼r Chunks
â”œâ”€â”€ prompt_protection.py    # Schutz vor Prompt-Injection
â”œâ”€â”€ logging_utils.py        # Logging-Setup & Event-Logging
â”œâ”€â”€ utils.py                # Hilfsfunktionen (z.B. Chatverlauf speichern)
â”œâ”€â”€ ... weitere Tools & Tests

data/, output/, metadata/   # Arbeits- & Ergebnisverzeichnisse
logs/                       # Logging-Ausgaben & Prompt-Injection-Logs
prompts/Chatverlauf/        # Persistenter Chatverlauf
```

## âš™ï¸ Setup & Nutzung

1. **AbhÃ¤ngigkeiten installieren**
   ```powershell
   pip install -r requirements.txt
   ```
2. **.env anlegen** (siehe `.env.example`)
3. **ChromaDB aufbauen**
   ```powershell
   python scripts/build_chroma_db.py
   ```
4. **Streamlit-App starten**
   ```powershell
   streamlit run scripts/app.py
   ```
5. **Ollama-Server** (z.B. Mistral) muss lokal laufen

## ğŸ§ª Testen

```powershell
pytest tests/
```

## ğŸ›¡ï¸ Sicherheit & Logging
- Schutz vor Prompt-Injection (regel- und LLM-basiert)
- Logging aller kritischen Events und verdÃ¤chtigen Eingaben
- Persistenter Chatverlauf fÃ¼r Nachvollziehbarkeit

## ğŸ“ Beispiel-Workflow

1. **Eigene Quellen vorbereiten**
   - Lege deine PDF- oder Textdateien im Ordner `data/pdf_data/` ab.

2. **Texte chunking & Embeddings erstellen**
   - Starte das Skript, um die Datenbank zu bauen:
     ```powershell
     python scripts/build_chroma_db.py
     ```
   - Alternativ: Nutze die Einzel-Skripte fÃ¼r Feinschliff (z.B. `chunk_texts.py`, `embed_chunks.py`).

3. **.env konfigurieren**
   - Kopiere `.env.example` zu `.env` und passe ggf. Pfade/Modelle an.

4. **Ollama-Server starten**
   - Stelle sicher, dass Ollama (z.B. Mistral) lokal lÃ¤uft:
     ```powershell
     ollama run mistral
     ```

5. **Streamlit-App starten**
   - Starte die Chat-OberflÃ¤che:
     ```powershell
     streamlit run scripts/app.py
     ```

6. **Chatten & Ergebnisse prÃ¼fen**
   - Stelle Fragen im Web-Interface.
   - Relevante Quellen werden automatisch gesucht und in die Antwort eingebunden.
   - Der Chatverlauf wird gespeichert (`prompts/Chatverlauf/`).

7. **Logs & Sicherheit prÃ¼fen**
   - VerdÃ¤chtige Eingaben werden in `logs/prompt_injection/` protokolliert.
   - Fehler und Systemereignisse findest du in `logs/fmgpt.log`.

8. **Tests ausfÃ¼hren (optional)**
   - PrÃ¼fe die Codebasis mit:
     ```powershell
     pytest tests/
     ```

---

> **Tipp:** FÃ¼r grÃ¶ÃŸere Datenmengen oder Anpassungen kannst du die Skripte im `scripts/`-Ordner flexibel kombinieren oder erweitern.



